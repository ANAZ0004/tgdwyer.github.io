---
layout: page
title: "Levels of Abstraction"
permalink: /levelsofabstraction/
---

## Learning Outcomes

- Understand the motivation for different programming paradigms: to abstract machine operation into human understandable and composable programs
- Understand the difference between *syntax* the textual symbols and grammatical rules of a program, and *semantics* the meaning of what is computed
- Understand that there are different models of computation upon which different programming languages are based, including machine models such as Turing Machines and von Neumann architecture, and the Lambda Calculus based on mathematical functions.

## Syntax versus Semantics

As a branch of Computer Science, the theory and practice of programming has grown from a very practical need: to create tools to help us get computers to perform useful and often complex tasks.  Programming languages are tools, that are designed and built by people to make life easier in this endeavour.  Furthermore, they are rapidly evolving tools that have grown in subtlety and complexity over the many decades to take advantage of changing computer hardware and to take advantage of different ways to model computation.

An important distinction to make when considering different programming languages, is between syntax and semantics. The syntax of a programming language is the set of symbols and rules for combining them (the grammar) into a correctly structured program.  These rules are often arbitrary and chosen for historical reasons, ease of implementation or even aesthetics.  An example of a syntactic design choice is the use of indentation to denote block structure or symbols such as “BEGIN” and “END” or “{” and “}”.

Example: functions in python and C that are syntactically different, but semantically identical:
```python
# python code
def sumTo(n):
   sum = 0
   for i in range(0,n):
       sum = sum + i
   return sum
```

```c
// C code:
int sumTo(int n) {
   int sum = 0;
   for(int i = 0; i < n; i++) {
       sum += i;
   }
   return sum;
}
```

By contrast, the “semantics” of a programming language relate to the meaning of a program: how does it structure data?  How does it execute or evaluate?

In this course we will certainly be studying the syntax of the different languages we encounter.  It is necessary to understand syntax in order to correctly compose programs that a compiler or interpreter can make sense of.  The syntactic choices made by language designers are also often interesting in their own right and can have significant impact on the ease with which we can create or read programs.  However, arguably the more profound learning outcome from this course should be an appreciation for the semantics of programming and how different languages lend themselves to fundamentally different approaches to programming, with different abstractions for modelling problems and different ways of executing and evaluating.  Hence, we will be considering several languages that support quite different programming paradigms.

For example, as we move forward we will see that C programs vary from the underlying machine language mostly syntactically.  C abstracts certain details and particulars of the machine architecture and has much more efficient syntax than Assembly language, but the semantics of C are not so far removed from Assembler - especially modern assembler that supports procedures and conveniences for dealing with arrays.

By contrast, Haskell and MiniZinc (which we will be exploring in the second half of this course) represent quite different paradigms where the underlying machine architecture, and even the mode of execution, is significantly abstracted away.  MiniZinc, in particular, is completely declarative in the sense that the programmer’s job is to define and model the constraints of a problem.  The approach to finding the solution, in fact any algorithmic details, are completely hidden.

One other important concept that we try to convey in this course is that while some languages are engineered to support particular paradigms (such as functional programming or logic programming), the ideas can be brought to many different programming languages.  For example, we begin learning about functional programming in JavaScript (actually TypeScript and EcmaScript 2017) and we will demonstrate that functional programming style is not only possible in this language, but brings with it a number of benefits.

Later, we will pivot from JavaScript (briefly) to PureScript, a haskell-like language that compiles to JavaScript.  We will see that, while PureScript syntax is very different to Javascript, the JavaScript generated by the PureScript compiler is not so different to the way we implemented functional paradigms manually in JavaScript.

Then we will dive a little-more deeply into a language that more completely “buys into” the functional paradigm: Haskell.  As well as having a syntax that makes functional programming very clean, the haskell compiler strictly enforces purity and makes the interesting choice of being lazy by default.

In summary, the languages we will study (in varying degrees of depth) will be Assembler, C/C++, JavaScript (ES2017 and TypeScript), PureScript, Haskell and MiniZinc, with JavaScript/TypeScript and Haskell being the main languages explored in problem sets and assignments.  Thus, this course will be a tour through programming paradigms that represent different levels of abstraction from the underlying machine architecture.  To begin, we spend just a little time at the level of least abstraction: the hardware itself.

## The Machine Level

Conceptually, modern computer architecture deviates little from the von Neumann model proposed in 1945 by Hungarian-American computer scientist John von Neumann.  The development of the von Neumann model occurred shortly after the introduction of the theoretical Turing Machine concept.

The von Neumann architecture was among the first to unify the concepts of data and programs.  That is, in a von Neumann architecture, a program is just data that can be loaded into memory.  A program is a list of instructions that read data from memory, manipulate it, and then write it back to memory.  This is much more flexible than previous computer designs which had stored the programs separately in a fixed (read-only) manner.

The classic von Neumann model looks like so:

![von Neumann Architecture](/levelsofabstraction/vonneumann.png)

At a high-level, standard modern computer architecture still fits within this model:

![Modern Computer Architecture](/levelsofabstraction/computerarchiture.png)

Programs run on an x86 machine according to the Instruction Execution Cycle:

- CPU fetches instruction from instruction queue, increments instruction pointer
- CPU decodes instruction and decides if it has operands
- If necessary, CPU fetches operands from registers or memory
- CPU executes the instruction
- If necessary, CPU stores result, sets status flags, etc.

Registers are locations on the CPU with very low-latency access due to their physical proximity to the execution engine.  Modern x86 processors also have 2 or 3 levels of cache memory physically on-board the CPU.  Accessing cache memory is slower than accessing registers (with all sorts of very complicated special cases) but still many times faster than accessing main memory.  The CPU handles the movement of instructions and data between levels of cache memory and main memory automatically, cleverly and—for the most part—transparently.  To cut a long story short, it can be very difficult to predict how long a particular memory access will take.  Probably, accessing small amounts of memory repeatedly will be cached at a high-level and therefore fast.

## Other Models of Computation

Turing Machines are a conceptual model of computation based on a physical analogy of tapes being written to and read from as they feed through a machine.  The operations written on the tape determine the machine's operation.  The von Neumann model of computation is has in common with Turing Machines that it follows an imperitive paradigm of sequential instruction execution, but it is a practical model upon which most modern computers base their architecture.

There are other models of computation that are also useful.  In particular, we will look at the [Lambda Calculus](/lambdacalculus), which was roughly a contemporary of these other models but based on mathematical functions, their application and composition.  We will see that while imperative programming languages are roughly abstractions of machine architecture, functional programming languages provide an alternative abstraction built upon the rules of the lambda calculus.  They provide advantages 
